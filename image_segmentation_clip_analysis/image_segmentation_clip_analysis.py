import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageDraw
import cv2
import sys
import os
from typing import List, Tuple, Dict
import argparse

# Add CLIP path
sys.path.append('/home/lumina/lumina/yukun/CLIP')
import clip

# SAM imports
try:
    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor
    SAM_AVAILABLE = True
except ImportError:
    print("Warning: SAM not available. Install with: pip install git+https://github.com/facebookresearch/segment-anything.git")
    SAM_AVAILABLE = False

class ImageSegmentationCLIPAnalyzer:
    """
    Image Segmentation and CLIP Text Matching Analyzer
    
    This tool performs:
    1. Image segmentation to extract objects (bottle, shoe, etc.)
    2. CLIP text encoding for multiple descriptions
    3. Feature similarity calculation and heatmap visualization
    """
    
    def __init__(self, clip_model_name="ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu", 
                 sam_checkpoint=None, gpu_id=0):
        # Set the device with specified GPU ID
        if device.startswith("cuda") and torch.cuda.is_available():
            self.device = f"cuda:{gpu_id}"
        else:
            self.device = device
            
        print(f"Loading CLIP model {clip_model_name} on {self.device}...")
        self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device=self.device)
        self.clip_model.eval()
        
        # SAM configuration
        self.sam_checkpoint = sam_checkpoint
        if SAM_AVAILABLE:
            print("SAM is available for advanced segmentation")
        else:
            print("SAM not available, will use fallback segmentation methods")
        
        # Common object descriptions for analysis
        self.default_descriptions = [
            "a water bottle",
            "a shoe", 
            "a cup",
            "a mug",
            "a sneaker",
            "a boot",
            "a plastic bottle",
            "a glass bottle",
            "a running shoe",
            "a leather shoe"
        ]
    
    def sam_segmentation(self, image_path: str, sam_model_type="vit_b", sam_checkpoint=None) -> Tuple[np.ndarray, np.ndarray]:
        """
        SAM (Segment Anything Model) based segmentation
        
        Args:
            image_path: Path to input image
            sam_model_type: SAM model type ('vit_b', 'vit_l', 'vit_h')
            sam_checkpoint: Path to SAM checkpoint file
            
        Returns:
            original_image: Original image as numpy array
            mask: Binary mask of the largest/most central segmented object
        """
        if not SAM_AVAILABLE:
            raise ImportError("SAM is not available. Please install it first.")
        
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not load image from {image_path}")
        
        original_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Default SAM checkpoint paths (you may need to download these)
        if sam_checkpoint is None:
            checkpoint_map = {
                "vit_b": "sam_vit_b_01ec64.pth",
                "vit_l": "sam_vit_l_0b3195.pth", 
                "vit_h": "sam_vit_h_4b8939.pth"
            }
            sam_checkpoint = checkpoint_map.get(sam_model_type, "sam_vit_b_01ec64.pth")
        
        try:
            # Load SAM model
            sam = sam_model_registry[sam_model_type](checkpoint=sam_checkpoint)
            sam.to(device=self.device)
            
            # Generate masks automatically
            mask_generator = SamAutomaticMaskGenerator(sam)
            masks = mask_generator.generate(original_image)
            
            if not masks:
                print("No masks generated by SAM, falling back to simple method")
                return self.simple_segmentation_fallback(image_path)
            
            # Select the best mask (largest area or most central)
            best_mask = self.select_best_mask(masks, original_image.shape[:2])
            
            return original_image, best_mask
            
        except Exception as e:
            print(f"SAM segmentation failed: {e}")
            print("Falling back to simple segmentation method")
            return self.simple_segmentation_fallback(image_path)
    
    def select_best_mask(self, masks: List[Dict], image_shape: Tuple[int, int]) -> np.ndarray:
        """
        Select the best mask from SAM output
        
        Args:
            masks: List of mask dictionaries from SAM
            image_shape: (height, width) of the image
            
        Returns:
            best_mask: Binary mask of the selected object
        """
        if not masks:
            return np.zeros(image_shape, dtype=np.uint8)
        
        height, width = image_shape
        center_x, center_y = width // 2, height // 2
        
        best_mask = None
        best_score = -1
        
        for mask_data in masks:
            mask = mask_data['segmentation'].astype(np.uint8)
            area = mask_data['area']
            
            # Calculate centrality score (how close the mask center is to image center)
            y_coords, x_coords = np.where(mask)
            if len(x_coords) == 0:
                continue
                
            mask_center_x = np.mean(x_coords)
            mask_center_y = np.mean(y_coords)
            
            # Distance from image center (normalized)
            center_dist = np.sqrt((mask_center_x - center_x)**2 + (mask_center_y - center_y)**2)
            max_dist = np.sqrt(center_x**2 + center_y**2)
            centrality = 1 - (center_dist / max_dist)
            
            # Area score (normalized)
            area_score = area / (height * width)
            
            # Combined score (balance between area and centrality)
            score = 0.6 * area_score + 0.4 * centrality
            
            # Prefer masks that are not too small or too large
            if 0.01 < area_score < 0.8 and score > best_score:
                best_score = score
                best_mask = mask
        
        if best_mask is None:
            # Fallback: use the largest mask
            best_mask = max(masks, key=lambda x: x['area'])['segmentation'].astype(np.uint8)
        
        return best_mask
    
    def simple_segmentation_fallback(self, image_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """
        Fallback segmentation method when SAM fails
        
        Args:
            image_path: Path to input image
            
        Returns:
            original_image: Original image as numpy array
            mask: Binary mask of segmented object
        """
        # Load image
        image = cv2.imread(image_path)
        original_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        height, width = image.shape[:2]
        
        # Use GrabCut as fallback
        mask = np.zeros((height, width), np.uint8)
        bgd_model = np.zeros((1, 65), np.float64)
        fgd_model = np.zeros((1, 65), np.float64)
        
        # Define rectangle around the object (center region)
        rect = (width//4, height//4, width//2, height//2)
        
        cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)
        
        # Create final mask
        mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')
        
        return original_image, mask2
    
    def extract_object(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """
        Extract object from image using mask
        
        Args:
            image: Original image
            mask: Binary mask
            
        Returns:
            extracted_object: Image with background removed
        """
        # Create 3-channel mask
        mask_3d = np.stack([mask] * 3, axis=-1)
        
        # Apply mask to image
        extracted = image * mask_3d
        
        # Set background to white
        background = np.ones_like(image) * 255
        extracted_object = np.where(mask_3d, extracted, background)
        
        return extracted_object.astype(np.uint8)
    
    def encode_texts(self, descriptions: List[str]) -> torch.Tensor:
        """
        Encode text descriptions using CLIP
        
        Args:
            descriptions: List of text descriptions
            
        Returns:
            text_features: Encoded text features
        """
        text_tokens = clip.tokenize(descriptions).to(self.device)
        
        with torch.no_grad():
            text_features = self.clip_model.encode_text(text_tokens)
            text_features = F.normalize(text_features, dim=-1)
        
        return text_features
    
    def encode_image(self, image: np.ndarray) -> torch.Tensor:
        """
        Encode image using CLIP
        
        Args:
            image: Input image as numpy array
            
        Returns:
            image_features: Encoded image features
        """
        # Convert to PIL Image
        pil_image = Image.fromarray(image)
        
        # Preprocess for CLIP
        image_tensor = self.clip_preprocess(pil_image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.clip_model.encode_image(image_tensor)
            image_features = F.normalize(image_features, dim=-1)
        
        return image_features
    
    def calculate_similarities(self, image_features: torch.Tensor, text_features: torch.Tensor) -> np.ndarray:
        """
        Calculate cosine similarities between image and text features
        
        Args:
            image_features: Encoded image features
            text_features: Encoded text features
            
        Returns:
            similarities: Similarity scores
        """
        similarities = torch.matmul(image_features, text_features.T)
        return similarities.cpu().numpy().flatten()
    
    def create_heatmap_visualization(self, descriptions: List[str], similarities: np.ndarray, 
                                   title: str = "CLIP Text-Image Similarity Heatmap") -> plt.Figure:
        """
        Create heatmap visualization of similarities
        
        Args:
            descriptions: Text descriptions
            similarities: Similarity scores
            title: Plot title
            
        Returns:
            fig: Matplotlib figure
        """
        # Create figure
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        # Reshape similarities for heatmap (1 x n)
        sim_matrix = similarities.reshape(1, -1)
        
        # Create heatmap
        sns.heatmap(sim_matrix, 
                   xticklabels=descriptions,
                   yticklabels=["Similarity"],
                   annot=True, 
                   fmt='.3f',
                   cmap='YlOrRd',
                   cbar_kws={'label': 'Cosine Similarity'},
                   ax=ax)
        
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlabel('Text Descriptions', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        
        return fig
    
    def create_comprehensive_visualization(self, original_image: np.ndarray, mask: np.ndarray, 
                                         extracted_object: np.ndarray, descriptions: List[str], 
                                         similarities: np.ndarray, image_path: str) -> plt.Figure:
        """
        Create comprehensive visualization showing all steps
        
        Args:
            original_image: Original input image
            mask: Segmentation mask
            extracted_object: Extracted object image
            descriptions: Text descriptions
            similarities: Similarity scores
            image_path: Path to original image
            
        Returns:
            fig: Matplotlib figure
        """
        fig = plt.figure(figsize=(20, 12))
        
        # Original image
        ax1 = plt.subplot(2, 3, 1)
        plt.imshow(original_image)
        plt.title('Original Image', fontsize=14, fontweight='bold')
        plt.axis('off')
        
        # Segmentation mask
        ax2 = plt.subplot(2, 3, 2)
        plt.imshow(mask, cmap='gray')
        plt.title('Segmentation Mask', fontsize=14, fontweight='bold')
        plt.axis('off')
        
        # Extracted object
        ax3 = plt.subplot(2, 3, 3)
        plt.imshow(extracted_object)
        plt.title('Extracted Object', fontsize=14, fontweight='bold')
        plt.axis('off')
        
        # Similarity bar chart
        ax4 = plt.subplot(2, 3, (4, 6))
        colors = plt.cm.YlOrRd(similarities / similarities.max())
        bars = ax4.bar(range(len(descriptions)), similarities, color=colors)
        ax4.set_xlabel('Text Descriptions', fontsize=12)
        ax4.set_ylabel('Cosine Similarity', fontsize=12)
        ax4.set_title('CLIP Text-Image Similarity Scores', fontsize=14, fontweight='bold')
        ax4.set_xticks(range(len(descriptions)))
        ax4.set_xticklabels(descriptions, rotation=45, ha='right')
        
        # Add value labels on bars
        for i, (bar, sim) in enumerate(zip(bars, similarities)):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{sim:.3f}', ha='center', va='bottom', fontsize=10)
        
        # Highlight the highest similarity
        max_idx = np.argmax(similarities)
        bars[max_idx].set_edgecolor('red')
        bars[max_idx].set_linewidth(3)
        
        plt.suptitle(f'Image Segmentation and CLIP Analysis\nImage: {os.path.basename(image_path)}', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    def analyze_image(self, image_path: str, descriptions: List[str] = None, 
                     use_sam: bool = True, sam_model_type: str = "vit_b", save_results: bool = True) -> Dict:
        """
        Complete analysis pipeline
        
        Args:
            image_path: Path to input image
            descriptions: List of text descriptions (uses default if None)
            use_sam: Whether to use SAM for segmentation (fallback to simple methods if False or unavailable)
            sam_model_type: SAM model type ('vit_b', 'vit_l', 'vit_h')
            save_results: Whether to save visualization results
            
        Returns:
            results: Dictionary containing analysis results
        """
        if descriptions is None:
            descriptions = self.default_descriptions
        
        print(f"Analyzing image: {image_path}")
        segmentation_method = f"SAM ({sam_model_type})" if use_sam and SAM_AVAILABLE else "Fallback (GrabCut)"
        print(f"Using segmentation method: {segmentation_method}")
        print(f"Text descriptions: {descriptions}")
        
        # Step 1: Segmentation
        print("\n1. Performing image segmentation...")
        if use_sam and SAM_AVAILABLE:
            original_image, mask = self.sam_segmentation(image_path, sam_model_type, self.sam_checkpoint)
        else:
            original_image, mask = self.simple_segmentation_fallback(image_path)
        
        # Step 2: Extract object
        print("2. Extracting object...")
        extracted_object = self.extract_object(original_image, mask)
        
        # Step 3: Encode texts and image
        print("3. Encoding texts and image with CLIP...")
        text_features = self.encode_texts(descriptions)
        image_features = self.encode_image(extracted_object)
        
        # Step 4: Calculate similarities
        print("4. Calculating similarities...")
        similarities = self.calculate_similarities(image_features, text_features)
        
        # Step 5: Find best match
        best_match_idx = np.argmax(similarities)
        best_match = descriptions[best_match_idx]
        best_score = similarities[best_match_idx]
        
        print(f"\nBest match: '{best_match}' (similarity: {best_score:.3f})")
        
        # Step 6: Create visualizations
        print("5. Creating visualizations...")
        
        # Comprehensive visualization
        comp_fig = self.create_comprehensive_visualization(
            original_image, mask, extracted_object, descriptions, similarities, image_path
        )
        
        # Heatmap visualization
        heatmap_fig = self.create_heatmap_visualization(
            descriptions, similarities, 
            f"CLIP Similarity Heatmap - Best: {best_match}"
        )
        
        # Save results if requested
        if save_results:
            base_name = os.path.splitext(os.path.basename(image_path))[0]
            comp_fig.savefig(f"{base_name}_comprehensive_analysis.png", dpi=300, bbox_inches='tight')
            heatmap_fig.savefig(f"{base_name}_similarity_heatmap.png", dpi=300, bbox_inches='tight')
            
            # Save extracted object
            extracted_pil = Image.fromarray(extracted_object)
            extracted_pil.save(f"{base_name}_extracted_object.png")
            
            print(f"\nResults saved:")
            print(f"- {base_name}_comprehensive_analysis.png")
            print(f"- {base_name}_similarity_heatmap.png")
            print(f"- {base_name}_extracted_object.png")
        
        # Show plots
        plt.show()
        
        # Prepare results
        results = {
            'image_path': image_path,
            'descriptions': descriptions,
            'similarities': similarities,
            'best_match': best_match,
            'best_score': best_score,
            'original_image': original_image,
            'mask': mask,
            'extracted_object': extracted_object,
            'segmentation_method': segmentation_method
        }
        
        return results

def main():
    parser = argparse.ArgumentParser(description='Image Segmentation and CLIP Analysis Tool with SAM')
    parser.add_argument('--image', type=str, required=True, help='Path to input image')
    parser.add_argument('--descriptions', type=str, nargs='+', 
                       help='Custom text descriptions (space-separated)')
    parser.add_argument('--no-sam', action='store_true', help='Do not use SAM, use fallback segmentation')
    parser.add_argument('--sam-model', type=str, default='vit_b', 
                       choices=['vit_b', 'vit_l', 'vit_h'],
                       help='SAM model type')
    parser.add_argument('--sam-checkpoint', type=str, help='Path to SAM checkpoint file')
    parser.add_argument('--no-save', action='store_true', help='Do not save results')
    parser.add_argument('--clip-model', type=str, default='ViT-B/32',
                       help='CLIP model to use')
    parser.add_argument('--gpu', type=int, default=0,
                       help='GPU ID to use (default: 0)')
    
    args = parser.parse_args()
    
    # Initialize analyzer
    analyzer = ImageSegmentationCLIPAnalyzer(
        clip_model_name=args.clip_model,
        sam_checkpoint=args.sam_checkpoint,
        gpu_id=args.gpu
    )
    
    # Run analysis
    results = analyzer.analyze_image(
        image_path=args.image,
        descriptions=args.descriptions,
        use_sam=not args.no_sam,
        sam_model_type=args.sam_model,
        save_results=not args.no_save
    )
    
    print("\n" + "="*50)
    print("ANALYSIS COMPLETE")
    print("="*50)
    print(f"Best matching description: '{results['best_match']}'")
    print(f"Similarity score: {results['best_score']:.3f}")
    print("\nAll similarity scores:")
    for desc, sim in zip(results['descriptions'], results['similarities']):
        print(f"  {desc}: {sim:.3f}")

if __name__ == "__main__":
    # Example usage when run directly
    if len(sys.argv) == 1:
        print("Image Segmentation and CLIP Analysis Tool with SAM")
        print("\nUsage examples:")
        print("python image_segmentation_clip_analysis.py --image /path/to/your/image.jpg")
        print("python image_segmentation_clip_analysis.py --image /path/to/your/image.jpg --descriptions 'a red shoe' 'a blue bottle'")
        print("python image_segmentation_clip_analysis.py --image /path/to/your/image.jpg --sam-model vit_l")
        print("python image_segmentation_clip_analysis.py --image /path/to/your/image.jpg --no-sam")
        print("python image_segmentation_clip_analysis.py --image /path/to/your/image.jpg --gpu 1")
        print("\nSAM Installation:")
        print("pip install git+https://github.com/facebookresearch/segment-anything.git")
        print("\nSAM Checkpoints (download to current directory):")
        print("- vit_b: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth")
        print("- vit_l: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth")
        print("- vit_h: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth")
        print("\nFor help: python image_segmentation_clip_analysis.py --help")
    else:
        main()